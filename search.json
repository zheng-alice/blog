[
  {
    "objectID": "posts/5-anomaly.html",
    "href": "posts/5-anomaly.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Methods for detecting and treating outliers/anomalies are hard to categorize as supervised/unsupervised or as relating more so to clustering, regression, or classification. Outliers and anomalies appear all over the place, in all kinds of data and frameworks - and in each case we need to deal with them accordingly. For instance, we saw that certain clustering methods can label points as outliers; while they can thus be applied to, say, labeled classification data opaquely and even only to a select label, this isn‚Äôt likely to be as effective as considering all classes in unison. Additionally, neither would apply well to dealing with outliers in regression, which is what we‚Äôll focus on first."
  },
  {
    "objectID": "posts/5-anomaly.html#outliers",
    "href": "posts/5-anomaly.html#outliers",
    "title": "Anomaly/outlier detection",
    "section": "Outliers",
    "text": "Outliers\nIt‚Äôs hard to give a definition that‚Äôs both precise and widely applicable, so for now let‚Äôs treat outliers as any kind of unexpected deviations from general trends. For instance, they could be points away from others or the prediction, points labeled differently, points generated by a different process.\n\n\n\nImage credit: Ajitesh Kumar.\n\n\nIn regression particularly, we can denote outliers as points that are very far from the otherwise fitted distribution. Equivalently, outliers could be considered to be points that significantly disrupt the fitted trend.\nLet‚Äôs try to illustrate this with some (artificial) data.\n\n\nRun linear regression on uncorrelated data.\nfrom pandas import DataFrame\nfrom numpy.random import normal, random, seed\nimport plotly.express as px\nseed(5805)\n\nn = 100\ndf = DataFrame({'x': normal(size=n-1) + random(n-1), 'y': normal(size=n-1)})\nfig = px.scatter(df, x='x', y='y', trendline='ols').show();\n\n\n\n\n                                                \nFigure¬†1: Uncorrelated data\n\n\n\nThe above shows a scatterplot of two variables that have no relationship; the fitted line through them is flat only because the deviation of x is higher.\nNow, watch what happens when we add a very distant point:\n\n\nRun linear regression on data with an outlier.\ndf.loc[n-1] = {'x': 10, 'y': 10, 'outlier': True}\nfig = px.scatter(df, x='x', y='y', trendline='ols')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nfig.show();\n\n\n\n\n                                                \nFigure¬†2: Regular OLS\n\n\n\nThe trendline has shifted towards the outlier. This isn‚Äôt good, as it could lead us to infer an erroneous correlation between the variables.\nJudging from our definition of outliers, one obvious solution already presents itself - why not just remove all points farther than some \\(s\\) standard deviations away from the regressed line? This actually works fine in practice, though with a caveat that the outlier may actually bring the line towards itself so much that it‚Äôs no longer considered an outlier. If we want to catch this, we‚Äôll need to fit a trend while excluding one point at a time from the data - which is fairly tedious.\nIn addition, maybe we don‚Äôt want to entirely remove the outlier, but just mitigate it to not be as disruptive. This is useful when it‚Äôs hard to justify to just throw away data for whichever reason, or when the appropriately adjusted effects of outliers are desirable.\n\nHuber loss\nThis specific loss function is used in robust regression, a type of regression less sensitive to outliers. It is defined as\n\\[\nL_\\delta(a) = \\begin{cases}\n    \\frac{1}{2} a^2 & \\text{if } |a| \\leq \\delta\\ ,\\\\\n    \\delta(|a| - \\frac{1}{2} \\delta) & \\text{otherwise}\\ ,\n\\end{cases}\n\\tag{1}\\]\nwhich as you may note is the \\(\\ell_2\\) loss when \\(|a| \\leq \\delta\\) and a scaled \\(\\ell_1\\) loss otherwise (joined together nicely). Specifically, it looks like this:\n\n\n\nImage credit: Wikipedia.\n\n\nAs such, \\(\\delta\\) denotes a boundary around the regression past which the points‚Äô effects start to scale more slowly. Huber loss has the advantage of functioning exactly as, or very close to the \\(\\ell_2\\) norm, but avoiding being strongly pulled by outliers.\n\n\nRun linear regression with Huber loss.\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import HuberRegressor\n\nfig = px.scatter(df, x='x', y='y')\nfig.update_traces(marker=dict(color=[\"#636efa\"] * (n-1) + [\"#EF553B\"]))\nX = df['x'].values.reshape(-1, 1)\nx_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny = df['y']\n\nmodel = HuberRegressor(epsilon=1)\nmodel.fit(X, y)\ny_fit = model.predict(x_range)\nfig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n    showlegend=False, line=dict(color=\"#636efa\"))).show();\n\n\n\n\n                                                \nFigure¬†3: OLS with Huber loss\n\n\n\nAs seen above, the erroneous slope of the line was just about halved. The outlier thus still has an effect on the regression, just not as pronounced."
  },
  {
    "objectID": "posts/5-anomaly.html#changepoint-detection",
    "href": "posts/5-anomaly.html#changepoint-detection",
    "title": "Anomaly/outlier detection",
    "section": "Changepoint detection",
    "text": "Changepoint detection\nThe second example I wanted to showcase deals specifically with anomaly detection, and applies to a different framework - online learning. We have seen it make an appearance when discussing multi-armed bandits, which needed to collect information and learn about the environment over time. Now, we‚Äôll consider an algorithm that must similarly receive data and make decisions sequentially.\n\nSetting\nSuppose you are given some time series data and tasked to determine whether the distribution producing the data has changed at some point. We may additionally consider a specific instance called step detection, where the data is promised to have a constant mean, until it potentially changes. So, you take this data and compute some statistics, maybe a windowed average to approximate the mean, and fairly easily narrow down the point where the shift happens. Fairly easy, right?\nNow, imagine that you‚Äôre given one point of this data at a time, and every time asked to make a decision on whether the change occurred. You can‚Äôt just wait until the end to announce your decision, as you‚Äôre retroactively penalized for every data point you consume after the change happens. You also can‚Äôt announce a change when you aren‚Äôt very certain, as you‚Äôre also penalized for doing so before the change actually happened. So, what to do?\nTurns out this problem is relevant in many real-world scenarios such as intruder detection, stock trading, and quality control. Today, we‚Äôll look at the following dataset, showing the yearly volumes of the Nile at Aswan.\n\n\nLoad and display the Nile dataset\nimport statsmodels.api as sm\nimport plotly.express as px\n\n# Time series data\nnile = sm.datasets.get_rdataset(\"Nile\").data\nfig = px.line(nile, x=\"time\", y=\"value\")\n\n# Ground truth change point\nfig.add_vline(x=1899, line_width=2, line_dash=\"dash\", line_color=\"black\")\n\n\n\n\n                                                \nFigure¬†4: Yearly volume of the Nile river at Aswan. A dam was built in 1898.\n\n\n\nThe dashed line seen above corresponds to the completion of a dam. If this change was unexpected, then identifying it and doing so as quickly as possible would be extremely crucial.\n\n\nCUSUM\nThis algorithm / statistical test is fairly old by most standards, being first described by Page (1954). It is quite frequent in statistical testing, though I still somehow wasn‚Äôt able to find a python implementation. For comparison, MATLAB has a built-in function cusum function, which I‚Äôll be using as a rough guide form a ground-up implementation.\n\nMotivation\nBut first, let‚Äôs look at the underlying idea of CUSUM. As the name suggests, this algorithm involves keeping track of a cumulative sum - let‚Äôs see one case where this could be useful.\nSuppose you‚Äôre flipping a coin, which originally starts with a \\(45%\\) of the outcome heads. As you do this, you keep a cumulative sum of the heads minus tails obtained so far, a number that seems to vary but is generally decreasing.\nHowever, after round \\(75\\) you suddenly see this number shoot up, which as you later find out, happened because the coin changed to \\(75%\\) chance heads after that round. At first you not be sure - the outcomes could just be random but unlikely. But after a few too many heads, it becomes clear that something‚Äôs up with the coin and you successfully stop.\n\n\nA visual example of changepoint detection\nfrom numpy import concatenate, cumsum, mean\n\n# Simulate coin flipping\nn, t = [75, 25], 50\np0, p1 = 0.45, 0.75\ncoin = concatenate((random((n[0], t)) &lt; p0, random((n[1], t)) &lt; p1))\ncumul = mean(cumsum(2*coin-1, axis=0), axis=1)\n\n# Display accumulated counts\navg0 = (2*p0 - 1) * n[0]\navg1 = (2*p1 - 1) * n[1] + avg0\nfig = px.scatter(y=cumul, labels={'x': 'Flip count', 'y': \"Accumulated (Heads - Tails)\"})\nfig.add_traces(go.Scatter(x=[n[0],n[0]+n[1]], y=[avg0, avg1],\n    mode=\"lines\", name=\"Changed\", line=dict(dash='dash')))\nfig.add_traces(go.Scatter(x=[0,n[0]], y=[0, avg0], mode=\"lines\", name=\"Original\"))\n\n\n\n\n                                                \nFigure¬†5: Cumulative outcomes of flipping a coin with changing probabilities.\n\n\n\nThe (\\(50\\)-trial average) path of the cumulative sum is shown above, revealing how the trajectory of said path is influenced by the probability change. One simple way to detect it would be to keep track of the lowest cusum count seen so far, and if the current round‚Äôs value exceeds that minimum by a certain threshold, then you terminate the algorithm.\n\n\nImplementation\nThe same idea applies to the actual algorithm, which keeps track of deviations from the expected mean. Its first step is to normalize the data via \\(Z_n = (X_n - \\bar{x}) / s\\). If \\(\\bar{x}\\) and \\(s\\) are not a priori available, they are calculated from the first few data points. Then, at each round, the algorithm keeps track of two drift values - one for each possible deviation direction. They are updated according to \\[\n\\begin{align}\n    H_n = \\max(0, H_{n-1} + Z_n - \\omega)\\ ,\\\\\n    L_n = \\min(0, L_{n-1} + Z_n + \\omega)\\ ,\n\\end{align}\n\\tag{2}\\]\nwhere we set \\(H_0 = L_0 = 0\\). Essentially, Equation¬†2 doesn‚Äôt let \\(H_t\\) drift into the negatives and \\(L_t\\) into the positives. At each round, both values are dampened by \\(\\omega\\). If either deviates by more than some threshold, the algorithm stops and detects a changepoint. Let‚Äôs implement this algorithm and run it on the Nile dataset.\n\n\nImplementation of the CUSUM algorithm\nimport numpy as np\n\ndef cusum(X, lim=5, w=0, mean=None, std=None, plot=True):\n    '''Detect changes in mean using the CUSUM test.\n\n    ARGUMENTS:\n    lim: number of standard deviations in drift to be detected.\n    w: number of standard deviations as damping coefficient.\n    mean: expected mean or None to compute from 25 leading samples.\n    std: expected standard deviation or None to compute from 25 leading samples.\n    plot: whether to display a visualization.\n\n    RETURNS:\n    index of the first detected change or None.\n    '''\n    # Estimate mean and standard deviation\n    if mean is None:\n        mean = np.mean(X[:25])\n    if std is None:\n        std = np.mean(X[:25])\n    Z = (X - mean) / std\n\n    n = len(Z)\n    H = np.zeros(n)\n    L = np.zeros(n)\n    for i, z in enumerate(Z):\n        H[i] = max(0, H[i-1] + z - w)\n        L[i] = min(0, L[i-1] + z + w)\n\n    idx_H = np.argmax(np.append(H, lim) &gt;= lim)\n    idx_L = np.argmax(np.append(L, -lim) &lt;= -lim)\n    idx = min(idx_H, idx_L)\n\n    if plot:\n        fig = go.Figure()\n        fig.add_traces(go.Scatter(y=H, name=\"Upper cumulative sum\"))\n        fig.add_traces(go.Scatter(y=L, name=\"Lower cumulative sum\"))\n        fig.add_hline(y=lim, line_dash='dash', line_color=\"#636efa\")\n        fig.add_hline(y=-lim, line_dash='dash', line_color=\"#EF553B\")\n        if idx &lt; n:\n            HL = H if idx_H &lt;= idx_L else L\n            fig.add_traces(go.Scatter(x=[idx], y=[HL[idx]],\n                mode=\"markers\", marker_symbol='circle-open', name=\"Changepoint\",\n                marker=dict(size=12, line_width=4, color='black')))\n        fig.show()\n\n    return idx if idx &lt; n else None\n\nchange = cusum(nile['value'], lim=2)\nprint(f\"Detected changepoint at year {nile['time'][change]}.\")\n\n\n\n\n                                                \nFigure¬†6: CUSUM control chart.\n\n\n\nDetected changepoint at year 1905.\n\n\nAs seen by the control chart, CUSUM predicts a change in the year 1905, only 6 years after the dam‚Äôs construction. This delay generally depends on the value of the threshold, which shouldn‚Äôt be lowered too much due to a higher chance of false positives. If on the contrary we set the threshold too high, the algorithm would still likely predict it, just after some additional time. If we additionally had wanted to find the year where the change occurred, we could trace the red line from the control chart back to when it first started drifting down, giving us pretty much the exact year construction was finished.\nIn summary, the CUSUM algorithm lets us detect a changepoint anomaly in time-series data in an online manner."
  },
  {
    "objectID": "posts/3-regression.html",
    "href": "posts/3-regression.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "Much like classification, it is a problem in supervised learning. Unlike classification, it predicts a continuous variable rather than a categorical one. This makes it applicable to a variety of problems where we need to model an unknown (dependent) variable based on one or many known (independent) ones. Some examples include:\n\nPredicting the rating based off of a text of an online review.\nComputing how much a self-driving car should accelerate/decelerate/turn (based on prior human data).\nHypothesis testing for whether one variable correlates with another in an experiment.\n\nWe create a (simplified, artificial) dataset along the lines of the last example. We make two cases: one with a linear trend and another exponential.\nNote: if you‚Äôre viewing this on mobile, you may need to turn your phone sideways to see the plots.\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure¬†1: A couple of artificial datasets\n\n\n\nWhile there is no ambiguity about the presence of a trend, we will still evaluate various methods on their ability to accurately fit it."
  },
  {
    "objectID": "posts/3-regression.html#regression",
    "href": "posts/3-regression.html#regression",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "Much like classification, it is a problem in supervised learning. Unlike classification, it predicts a continuous variable rather than a categorical one. This makes it applicable to a variety of problems where we need to model an unknown (dependent) variable based on one or many known (independent) ones. Some examples include:\n\nPredicting the rating based off of a text of an online review.\nComputing how much a self-driving car should accelerate/decelerate/turn (based on prior human data).\nHypothesis testing for whether one variable correlates with another in an experiment.\n\nWe create a (simplified, artificial) dataset along the lines of the last example. We make two cases: one with a linear trend and another exponential.\nNote: if you‚Äôre viewing this on mobile, you may need to turn your phone sideways to see the plots.\n\n\nCreate artificial data\nimport plotly.express as px\nfrom numpy.random import rand, normal, seed\nfrom numpy import repeat, concatenate, power\nfrom pandas import DataFrame\nseed(5805)\n\n# Create artificial data\nn, r, s, b = 100, 100, 5, 10\nlin = repeat(rand(n, 1) * r, 2, axis=1) + normal(scale=s, size=(n, 2))\nexp = power(b, lin[:,1:2] / r) * r / b\nexp = concatenate((lin[:,0:1], exp), axis=1)\n\n# Plot data\nlin_df = DataFrame({'x': lin[:,0], 'y': lin[:,1]})\nexp_df = DataFrame({'x': exp[:,0], 'y': exp[:,1]})\npx.scatter(lin_df, x='x', y='y').show()\npx.scatter(exp_df, x='x', y='y').show();\n\n\n\n\n\n\n\n                                                \n(a) Linear growth\n\n\n\n\n\n                                                \n(b) Exponential growth\n\n\n\nFigure¬†1: A couple of artificial datasets\n\n\n\nWhile there is no ambiguity about the presence of a trend, we will still evaluate various methods on their ability to accurately fit it."
  },
  {
    "objectID": "posts/3-regression.html#linear-regression",
    "href": "posts/3-regression.html#linear-regression",
    "title": "Linear and nonlinear regression",
    "section": "Linear regression",
    "text": "Linear regression\nSo, what is it? As the name suggests, it‚Äôs when you regress with a line. A linear line. A line that is linear (i.e., straight and not nonlinear).\nLet‚Äôs go with that definition for now. We‚Äôll elaborate more on it later.\nBut, there are many possible ways to draw a (linear) line. How do we know which one is the best? Introducing‚Ä¶\n\nLeast squares\nDespite your best efforts to draw a straight line that passes though all points, you‚Äôll likely find that noisy data is rather adamant on making this impossible. In particular, the line only gives you two degrees of freedom to play with (slope and intercept), while each data point subtracts one. So, for sets of three or more points, unless they are perfectly collinear, you‚Äôll have to resort to a line that passes close (but not exactly through) most of the points. Such systems are also known as overdetermined. The result may look something like this:\n\n\n\nFigure¬†2: Visualization of residuals in linear regression. Image credit: Wikipedia.\n\n\nThe vertical green lines indicate residuals - the things you would want to avoid to get a good fit. Specifically, we wish to minimize the sum of some function of the residuals; common choices include the \\(\\ell_1\\) and \\(\\ell_2\\) norms.\nHere are a couple of physics-like interpretations that I like to think about when it comes to Figure¬†2:\n\nThe points being holes in the wall, through which strings are strung which connect to a stick on the front side and to a weight on the other. This is analogous to \\(\\ell_1\\) norm as the exerted force isn‚Äôt dependent on line‚Äôs distance away from the hole. The weights can be directly adjusted to model weighted data.\nThe points being pins fixed to the wall, which are connected via springs to a stick. The springs‚Äô resting point is whey they are fully compressed, so they pull the stick closer to the pins with more force the further it is, resembling \\(\\ell_2\\) norm. To model weighted data, the spring coefficients may be adjusted proportionally.\n\nExcept that the strings/springs can only be oriented vertically, and they are free to slide along the line, and everything except the weights is weightless, and‚Ä¶ Yeah, the analogy breaks down pretty fast. But its basic point stands: the points want the line to be close, so each pulls the line towards itself. As there isn‚Äôt a way to satisfy all points, the line ends up being ‚Äúbetween‚Äù them, oriented in the same direction as their trend.\n\n\nDerivation\nBut how do we actually find this optimal line? First, we should define our model more properly. Let‚Äôs suppose (for now) that we just want to regress a variable1 \\(Y\\) in terms of one other variable \\(X_1\\). Our model would have two coefficients; \\(\\beta_1\\) for the effect of \\(X_1\\) and \\(\\beta_0\\) for the intercept. Denoting the data sampled from the variables by \\(\\{x_{i,1}, y_i\\}_{i=1}^n\\), we would then approximate2 \\(y_i \\sim \\beta_0 + \\beta_1 x_{i,1}\\).\nSuppose instead that we have \\(d\\) variables \\(X_1, X_2, \\ldots, X_d\\). Accordingly, the updated formula would be \\[\ny_i \\sim \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\ldots + \\beta_d x_{i,d}\n= \\sum_{j=0}^d x_{i,j} \\beta_j\\ ,\n\\tag{1}\\]\nwhen we set \\(x_{i,0} = 1\\). Collecting \\(y_i\\) into an \\(n\\)-dimensional column vector \\(y\\), \\(\\beta_j\\) into a \\((d+1)\\)-dimensional column vector \\(\\beta\\), and \\(x_{i,j}\\) into an \\(n\\)-by-\\((d+1)\\) matrix, this lets us write Equation¬†1 as \\(y \\sim X \\beta\\). To make it an equality, we add an error term like \\[\ny = X \\beta + \\epsilon\\ ,\n\\tag{2}\\]\nwhere the added \\(\\epsilon\\) is a vector of the same residuals featured in Figure¬†2. Our goal becomes to solve Equation¬†2 for the optimal \\(\\hat{\\beta}\\) that minimizes the \\(2\\)-norm \\(||\\epsilon||\\).\nSure, but how do we actually do that? Turns out, we can just prepend \\(X^\\top\\) to both sides and get rid of the \\(\\epsilon\\). Specifically, we obtain \\(X^\\top y = X^\\top X \\hat{\\beta}\\), commonly known as the system of normal equations. This provides a closed-form solution3\n\\[\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y = X^\\dagger y\\ ,\n\\tag{3}\\]\nwhere \\(X^\\dagger\\) denotes the left pseudoinverse of \\(X\\), a generalization of the matrix inverse partially dedicated to least squares. As a side note, solving Equation¬†3 in MATLAB is as simple as X\\y.\nOk, but why does this work? One relevant observation is that prepending \\(X^\\top\\) reduces the system from \\(n\\) equations to \\(d+1\\), exactly matching the number of parameters in \\(\\beta\\) and thus making the problem no longer overconstrained. To see the reason in full detail though, let‚Äôs return back to the original optimization problem \\[\n\\begin{align}\n    \\text{minimize} \\quad& || \\epsilon ||\\ ,\\\\\n    \\text{such that} \\quad& y = X \\beta + \\epsilon\\ .\n\\end{align}\n\\tag{4}\\]\nThis is a convex optimization problem, more specifically an instance of quadratic programming. However, we will not solve it as such, as we‚Äôve just seen that it has a closed-form solution. First, note that \\(\\epsilon\\) is redundant - let‚Äôs remove to to simplify the problem to \\(\\hat{\\beta} = \\arg\\min_\\beta || y - X \\beta ||\\). What now? Let‚Äôs take the partial of the objective value with respect to \\(\\beta\\) (for a reason I will explain later) using matrix calculus - a tool that every machine learning theorist should have a run-in with at one point, methinks. Before we start though, we should square the objective - as this makes our lives easier without altering the optimal solution \\(\\hat{\\beta}\\). Alright, here we go!4\n\\[\n\\begin{align}\n    \\frac{\\partial}{\\partial \\beta} || y - X \\beta ||^2\n    &= \\frac{\\partial}{\\partial \\beta} ( y - X \\beta )^\\top ( y - X \\beta ) \\\\\n    &= 2 ( y - X \\beta )^\\top \\frac{\\partial}{\\partial \\beta} ( y - X \\beta ) \\\\\n    &= - 2 ( y - X \\beta )^\\top X\n\\end{align}\n\\tag{5}\\]\nOh, that was actually easier than I thought. All that‚Äôs left is a bit of elementary calculus. The whole reason we computed the partial is so we can set it to zero and obtain the problem‚Äôs critical points. Since this is a convex optimization problem, we know that any critical point we get is guaranteed to be an optimal solution. This gives us \\(X^\\top (y - X \\hat{\\beta}) = 0\\), which easily translates to the familiar normal equations \\(X^\\top X \\hat{\\beta} = X^\\top y\\) and then Equation¬†3, as needed.\nThe constraint obtained just now has a pretty interesting geometric interpretation. Specifically, it implies that \\(y - X \\hat{\\beta}\\) is orthogonal not only to the column space of \\(X\\), but also \\(X v\\) for any \\(v\\). As such, \\(y - X \\hat{\\beta}\\) is orthogonal to the linear subspace spanned by \\(X\\), and thus the shortest of any possible \\(y - X \\beta\\). In short, \\(X \\hat{\\beta}\\) is the orthogonal projection of \\(y\\) onto the column space of \\(X\\).\nA few additional remarks: least squares is an interesting case where the default estimator is equivalent to the MLE one. Lastly, Equation¬†3 may still be ill-conditioned if one of the variables in \\(X\\) is a scalar multiple of another, i.e., there is perfect multicollinearity. In practice this shouldn‚Äôt break the solver unless features are somehow duplicated, yet it may still make \\(\\hat{\\beta}\\) unstable in cases of high correlation. In such cases, it is best to avoid inferring effects from the coefficients of \\(\\hat{\\beta}\\).\n\n\nExamples\nAlright, let‚Äôs actually use this thing! Let‚Äôs take our fake data and try to fit some lines on it. Luckily, linear least squares is common enough for it to be ubiquitously implemented. The only case that we need special case for is introducing polynomial coefficients.\n\n\nCode for polynomial OLS\n# Taken from https://plotly.com/python/ml-regression/\nfrom numpy import linspace\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ndef plot(df, trend, **kwargs):\n    fig = px.scatter(df, x='x', y='y')\n    X = df['x'].values.reshape(-1, 1)\n    x_range = linspace(X.min(), X.max(), 100).reshape(-1, 1)\n    y = df['y']\n\n    y_fit = trend(X, x_range, y, **kwargs)\n    fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_fit,\n        showlegend=False, line=dict(color='black')))\n    return fig\n\ndef poly(X, x_range, y, degree=2):\n    poly = PolynomialFeatures(degree)\n    poly.fit(X)\n    X_poly = poly.transform(X)\n    x_range_poly = poly.transform(x_range)\n\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X_poly, y)\n    return model.predict(x_range_poly)\n\ndef show(fig):\n    fig.update_layout(\n            margin=dict(l=0, r=20, t=20, b=20),\n            height=380\n        ).show()\n\n\nThe rest are so common, you can get them by just passing one additional parameter trendline='ols' to a plotting function. Below, we use the ordinary least squares (OLS) regressor that we defined above with regular coefficients, polynomial coefficients, and a \\(\\log\\)-transform of the dependent variable.\n\n\nFit trendlines via ordinary least squares\nshow(px.scatter(lin_df, x='x', y='y', trendline='ols',\n    trendline_color_override='black'))\nshow(plot(exp_df, poly, degree=2))\nshow(px.scatter(exp_df, x='x', y='y', trendline='ols',\n    trendline_options=dict(log_y=True),\n    trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Ordinary least squares\n\n\n\n\n\n                                                \n(b) OLS of y ~ x^2 + x\n\n\n\n\n\n                                                \n(c) OLS of log(y) ~ x\n\n\n\nFigure¬†3: Trendlines via ordinary least squares.\n\n\n\nWait, where are the lines? The linear lines? Turns out our brief description of linear regression comes with a huge caveat: such regression is linear because of the linearity of its underlying models, not necessarily because it fits straight lines. As we have used variables (\\(x^2\\), \\(\\log(y)\\)) nonlinear with respect to the original data, the trendlines on the untransformed axes can inherit this nonlinearity. The latter \\(\\log\\)-transform is actually a fundamental concept in generalized linear models (GLMs), a type of linear regression models using a link function to map their linear model to nonlinear dependent variables. Despite this, GLMs are still considered to be linear - after all, they aren‚Äôt generalized nonlinear models (which are actually their own thing).\nAnd now, a word on the performance. The \\(y \\sim x\\) fit performs exactly as expected, nothing spectacular there. We won‚Äôt try it on the exponential data as its fit will be obviously bad. On the latter, the \\(y \\sim x^2 + x\\) fit performs reasonably well, though you may note it‚Äôs a bit flatter on the ends and wouldn‚Äôt generalize well outside the domain. Additionally, it‚Äôs evident that the exponential dataset suffers from heteroscedasticity - a phenomenon where points ‚Äúfan out‚Äù more for larger values. Though this can‚Äôt be easily seen above, it poses a problem for the polynomial fit, causing it to prioritize fitting the larger values more accurately at the cost of ignoring the lower values. For such data, we typically care about the relative residuals, which are perfectly captured by the \\(\\log\\)-transform. After all, the transformed data will be exactly the linear data, for which an OLS fit prioritizes all residuals equally and thus works extremely well."
  },
  {
    "objectID": "posts/3-regression.html#nonlinear-regression",
    "href": "posts/3-regression.html#nonlinear-regression",
    "title": "Linear and nonlinear regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\nSo, if we can fit all kinds of nonlinear-looking lines with linear regression, what then is nonlinear regression? Turns out, it covers a far more general predictors, ones that can be fit using virtually any parametrized function \\(f\\). In particular, we fit \\(y \\sim f(x, \\beta)\\) where \\(\\beta\\) and the parameters we need to find to fit the function.\nNote that this problem has virtually no structure - as such, the objective landscape may have many local minima or other unexpected properties that make convergence tough. Even if the optimal solution is found, it isn‚Äôt guaranteed to be an unbiased estimator. As the function \\(f\\) is provided as a black box, we do not have access to its derivative and thus cannot use gradient descent. Instead, common methods typically resort to some other sort of iterative optimization, or otherwise approximate the Jacobian using linear methods.\n\nExamples\nWe use the curve_fit function from scipy, which by default uses the Levenberg-Marquardt (LM) algorithm. I won‚Äôt go into the details of this algorithm, except to point that its (undampened) iteration step looks like \\((J^\\top J) \\delta = J^\\top (y - f(\\beta))\\). Now, isn‚Äôt that familiar?\nThe function that we will try to fit is \\[\nf(x, \\beta) \\coloneqq \\frac{\\beta_0 x}{\\beta_1 + x} + \\beta_2\\ ,\n\\tag{6}\\]\nwhich can be thought of as a parametrized version of \\(1/x\\). One troublesome point about LM is that it only converges to the nearest local minimum; as such, we must supply an already pretty accurate guess to get anywhere useful. The guess below specifically comes from manually messing around with the parameters in a graphing calculator.\n\n\nCode for KNN regression\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndef knn(X, x_range, y, k=20, weights='uniform'):\n    knn_uni = KNeighborsRegressor(10, weights=weights)\n    knn_uni.fit(X, y)\n    return knn_uni.predict(x_range.reshape(-1, 1))\n\n\n\n\nFit trendlines via miscellaneous methods\nfrom scipy.optimize import curve_fit\n\n# Nonlinear regression\nf = lambda x, a, b, c: a * x / (b + x) + c\ndef nonlinear(X, x_range, y, f, init=None):\n    args, _ = curve_fit(f, exp_df['x'], exp_df['y'], p0=init)\n    return f(x_range, *args).squeeze()\nshow(plot(exp_df, nonlinear, f=f, init=[-90, -200, 10]))\n\n# Non-parametric regression\nshow(plot(exp_df, knn, k=20))\nshow(px.scatter(exp_df, x='x', y='y', trendline='lowess',\n   trendline_color_override='black'))\n\n\n\n\n\n\n\n                                                \n(a) Nonlinear least squares\n\n\n\n\n\n                                                \n(b) KNN Regression\n\n\n\n\n\n                                                \n(c) LOWESS\n\n\n\nFigure¬†4: Trendlines via other regression methods.\n\n\n\nThe nonlinear fit, seen on the left, is fairly good - probably on par with the polynomial fit. While all the trouble we have gone through to get this okay fit may not seem worth it, it‚Äôs extremely useful in cases where the function \\(f\\) is complex but known ahead of time - such as in mathematical modeling. I have taken a class on just that, and we have used nonlinear curve fitting tools like this all the time. One particularly memorable example involved deriving the dynamics of an outbreak of an infectious disease with an SEIRS model and then using nonlinear regression to fit its parameters.\nYou‚Äôll notice that I threw in a couple of bonus fits there; both are examples of a different kind of regression,\n\n\nNonparametric regression\nWhereas the models up to this point all optimized for some parameters \\(\\beta\\) (and thus live within parametric regression), the two fits just shown have no such parameters. Instead, their fit is entirely dependent on the surrounding data. For instance, both KNN regression and LOWESS use some sort of weighted average of the surrounding points either directly or to fit a local polynomial spline.\nI will depart with just one last example - a visual reminder that all these methods can do multivariate regression (as mentioned in the derivation of linear least squares). This method Support Vector Regression (SVR), that, if MATLAB is to be believed, is considered a nonparametric method.\n\n\nFit a multivariate regression model\n# Taken from https://plotly.com/python/ml-regression/\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.svm import SVR\n\nmesh_size = .02\nmargin = 0\n\ndf = px.data.iris()\n\nX = df[['sepal_width', 'sepal_length']]\ny = df['petal_width']\n\n# Condition the model on sepal width and length, predict the petal width\nmodel = SVR(C=1.)\nmodel.fit(X, y)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X.sepal_width.min() - margin, X.sepal_width.max() + margin\ny_min, y_max = X.sepal_length.min() - margin, X.sepal_length.max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Run model\npred = model.predict(np.c_[xx.ravel(), yy.ravel()])\npred = pred.reshape(xx.shape)\n\n# Generate the plot\nfig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')\nfig.update_traces(marker=dict(size=5))\nfig.add_traces(go.Surface(x=xrange, y=yrange, z=pred, name='pred_surface', opacity=0.75))\nfig.show()\n\n\n\n\n                                                \nFigure¬†5: Support Vector Regression on the Iris dataset.\n\n\n\nWhoa, \\(\\uparrow\\) cool plot! Drag your mouse/finger on it to rotate.\nYou may recognize the above points as the Iris dataset, minus its labels. While it‚Äôs generally better-suited for classification, fits such as the one seen above could be useful for predicting a variable or filling in missing data in cases where labels aren‚Äôt available."
  },
  {
    "objectID": "posts/3-regression.html#footnotes",
    "href": "posts/3-regression.html#footnotes",
    "title": "Linear and nonlinear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs hinted by this notation, \\(X\\), \\(y\\) and later \\(\\epsilon\\) can be thought of either as a random vector/variables or matrix/vector samples of these. Here, we will stick to the latter as it directly corresponds to the provided data and makes things somewhat simpler.‚Ü©Ô∏é\nThis \\(\\sim\\) notation isn‚Äôt entirely precise, but can be interpreted roughly as \\(\\approx\\). It‚Äôs meant to imitate R‚Äôs formula notation like glm(y ~ x, ...) or glm(y ~ ., ...).‚Ü©Ô∏é\nA tip from numerical analysis: don‚Äôt compute \\((X^\\top X)^{-1}\\) directly as that is rather slow and imprecise. Instead, solve \\(Ax = b\\) with \\(A = X^\\top X\\) and \\(b = X^\\top y\\).‚Ü©Ô∏é\nUsing the numerator layout.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/1-probability.html",
    "href": "posts/1-probability.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "You may remember them from your intro to statistics class. They‚Äôre less like a number and more like a distribution. Less like -1¬∞C (the temperature here, now) and more like -3.7¬∞C to 7.2¬∞C (the daily mean temperature in Blacksburg in December). Less like a particular experiment outcome and more like the experiment itself. Basically, a number that isn‚Äôt fixed and can vary.\nPretty simple, right? Here‚Äôs the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\).\n\nUh‚Ä¶ what? Let‚Äôs break this down a little.\n\n\nLet \\(\\Omega = \\{üò∏,üòø\\}\\) denote the outcomes of an experiment, with associated probabilities \\(\\mathrm{P}(üò∏) = 0.5\\) and \\(\\mathrm{P}(üòø) = 0.5\\). What is the average outcome of this experiment? Certainly, it can‚Äôt be \\(0.5üò∏ + 0.5üòø\\); that quantity doesn‚Äôt make sense. We instead need something we can count, such as the number of happy cats. This we can denote by a random variable \\(X\\), something that maps event outcomes in \\(\\Omega\\) to measurable quantities in \\(E\\), which in this case is \\(\\{0, 1\\}\\). Let \\(X(üò∏) = 1\\) and \\(X(üòø) = 0\\); then, to get the probability of a particular outcome in \\(E\\) we must find all entries in \\(\\Omega\\) that map to it and sum their probabilities. This can be written as \\[\n\\mathrm{P}(X = 1) = \\mathrm{P}(\\{ \\omega \\in \\Omega : X(\\omega) = 1 \\}) = \\mathrm{P}(üò∏) = 0.5\\ ,\n\\tag{1}\\]\nand analogously for \\(\\mathrm{P}(X = 0) = \\mathrm{P}(üòø) = 0.5\\). We can thus evaluate the expected value as \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\Omega} x \\mathrm{P}(X = x) = 0.5 \\mathrm{P}(X = üò∏) + 0.5 \\mathrm{P}(X = üòø) = 0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5\\ ,\n\\tag{2}\\]\nmeaning that on average the experiment produces one half of a happy cat.\nThe reason for all the remaining clunkiness in the definition is that its formality stems from measure theory, a branch of probability theory that can deal with otherwise confusing situations like distributions that are part-discrete and part-continuous. For instance, we need to define \\(\\mathcal{F}\\) and \\(\\mathcal{E}\\) as collections of subsets of \\(\\Omega\\) and \\(E\\) since we can‚Äôt map events and values to probabilities directly (continuous distributions would have zero probability), and must instead map sets. When calculating \\(\\mathbb{E}[X]\\) we would then have to take the Lebesgue integral \\(\\int_\\Omega X dP\\).\nOne additional small note: we can arrange random variables \\(X_1, \\ldots, X_n\\) in a vector to obtain a random vector \\(X = \\begin{bmatrix} X_1, \\ldots, X_n \\end{bmatrix}^\\top\\). The only mathematical caveat is that the variables must act on the same probability and measurable spaces.\n\n\n\nIn short, I think the best way to remember what random variables are, is by what they aren‚Äôt. Specifically, they are not random, and they are not variables. They don‚Äôt themselves serve as a source of randomness, instead only taking as an input the outcomes of an already random experiment. They do not vary; despite potentially giving different values upon being sampled, the mathematical object they describe is fixed. Although, given the context in which they typically appear, I can certainly see the appeal of the name."
  },
  {
    "objectID": "posts/1-probability.html#random-variables",
    "href": "posts/1-probability.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "You may remember them from your intro to statistics class. They‚Äôre less like a number and more like a distribution. Less like -1¬∞C (the temperature here, now) and more like -3.7¬∞C to 7.2¬∞C (the daily mean temperature in Blacksburg in December). Less like a particular experiment outcome and more like the experiment itself. Basically, a number that isn‚Äôt fixed and can vary.\nPretty simple, right? Here‚Äôs the definition; taken from Wikipedia, which in order takes it from Fristedt and Gray (1996, 11).\n\nDefinition 1 (Random variable). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\((E, \\mathcal{E})\\) a measurable space. Then an \\((E, \\mathcal{E})\\)-valued random variable is a measurable function \\(X : \\Omega \\to E\\), which measures that, for every subset \\(B \\in \\mathcal{E}\\), its preimage is \\(\\mathcal{F}\\)-measurable; \\(X^{-1}(B) \\in \\mathcal{F}\\), where \\(X^{-1}(B) = \\{\\omega : X(\\omega) \\in B\\}\\).\n\nUh‚Ä¶ what? Let‚Äôs break this down a little.\n\n\nLet \\(\\Omega = \\{üò∏,üòø\\}\\) denote the outcomes of an experiment, with associated probabilities \\(\\mathrm{P}(üò∏) = 0.5\\) and \\(\\mathrm{P}(üòø) = 0.5\\). What is the average outcome of this experiment? Certainly, it can‚Äôt be \\(0.5üò∏ + 0.5üòø\\); that quantity doesn‚Äôt make sense. We instead need something we can count, such as the number of happy cats. This we can denote by a random variable \\(X\\), something that maps event outcomes in \\(\\Omega\\) to measurable quantities in \\(E\\), which in this case is \\(\\{0, 1\\}\\). Let \\(X(üò∏) = 1\\) and \\(X(üòø) = 0\\); then, to get the probability of a particular outcome in \\(E\\) we must find all entries in \\(\\Omega\\) that map to it and sum their probabilities. This can be written as \\[\n\\mathrm{P}(X = 1) = \\mathrm{P}(\\{ \\omega \\in \\Omega : X(\\omega) = 1 \\}) = \\mathrm{P}(üò∏) = 0.5\\ ,\n\\tag{1}\\]\nand analogously for \\(\\mathrm{P}(X = 0) = \\mathrm{P}(üòø) = 0.5\\). We can thus evaluate the expected value as \\[\n\\mathbb{E}[X] = \\sum_{x \\in \\Omega} x \\mathrm{P}(X = x) = 0.5 \\mathrm{P}(X = üò∏) + 0.5 \\mathrm{P}(X = üòø) = 0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5\\ ,\n\\tag{2}\\]\nmeaning that on average the experiment produces one half of a happy cat.\nThe reason for all the remaining clunkiness in the definition is that its formality stems from measure theory, a branch of probability theory that can deal with otherwise confusing situations like distributions that are part-discrete and part-continuous. For instance, we need to define \\(\\mathcal{F}\\) and \\(\\mathcal{E}\\) as collections of subsets of \\(\\Omega\\) and \\(E\\) since we can‚Äôt map events and values to probabilities directly (continuous distributions would have zero probability), and must instead map sets. When calculating \\(\\mathbb{E}[X]\\) we would then have to take the Lebesgue integral \\(\\int_\\Omega X dP\\).\nOne additional small note: we can arrange random variables \\(X_1, \\ldots, X_n\\) in a vector to obtain a random vector \\(X = \\begin{bmatrix} X_1, \\ldots, X_n \\end{bmatrix}^\\top\\). The only mathematical caveat is that the variables must act on the same probability and measurable spaces.\n\n\n\nIn short, I think the best way to remember what random variables are, is by what they aren‚Äôt. Specifically, they are not random, and they are not variables. They don‚Äôt themselves serve as a source of randomness, instead only taking as an input the outcomes of an already random experiment. They do not vary; despite potentially giving different values upon being sampled, the mathematical object they describe is fixed. Although, given the context in which they typically appear, I can certainly see the appeal of the name."
  },
  {
    "objectID": "posts/1-probability.html#concentration-bounds",
    "href": "posts/1-probability.html#concentration-bounds",
    "title": "Probability theory and random variables",
    "section": "Concentration bounds",
    "text": "Concentration bounds\nSuppose we have a random variable \\(X\\) that is promised to give values in \\([0,1]\\). We may study such an \\(X\\) via its parameters, such as its expected value \\(\\mathbb{E}[X]\\). While we may have access to that during mathematical analysis, in practice we typically have to rely on some data1 \\(\\{X_i\\}_{i=1}^n\\) sampled from \\(X\\) to compute statistics such as the sample mean \\(\\frac{1}{n} \\sum_{i=1}^n X_i\\). But, do we know how accurate our approximations are? For instance, can we find some \\(f\\) such that \\[\n\\Biggr \\lvert \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}[X] \\Biggr \\rvert &lt; f(n)\\ ?\n\\tag{3}\\]\n\nHoeffding‚Äôs inequality\nThis famous inequality provides one such bound.\n\nTheorem 1 (Hoeffding‚Äôs inequality). Let \\(X_1, \\ldots, X_n\\) be independent random variables where with probability \\(1\\), \\(X_i \\in [a_i, b_i]\\) (i.e., bounded). Let \\(R_i \\coloneqq b_i - a_i\\) be the range. Then for all \\(\\epsilon \\geq 0\\), \\[\nP \\left( \\Biggr \\lvert \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\Biggr \\rvert \\leq \\epsilon \\right) \\geq 2 \\exp \\left( -\\frac{2 \\epsilon^2}{\\sum_{i=1}^n R_i^2} \\right)\\ .\n\\tag{4}\\]\n\nWe can re-arrange the variables to obtain that for any fixed \\(\\delta \\in (0, 1]\\), with probability at least \\(1 - \\delta\\), \\[\n\\Biggr \\lvert \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\Biggr \\rvert \\leq \\sqrt{\\frac{\\sum_{i=1}^n R_i^2 \\log(2/\\delta)}{2}}\\ .\n\\tag{5}\\]\n\n\nApplication\nThe following example is provided by Ji (2022, Lecture 13):\nSuppose you are given a coin that lands on heads with probability \\(\\mu\\). Every time you flip it, you get an outcome \\(X_i\\) - a Bernoulli distribution with mean \\(\\mu\\). After \\(n\\) flips, you compute the sample mean \\(\\hat{\\mu}_n \\coloneqq \\frac{1}{n} \\sum_{i=1}^n X_i\\). By Hoeffding‚Äôs inequality, with probability at least \\(1-\\delta\\), \\[\n| \\hat{\\mu}_n - \\mu | \\leq \\sqrt{\\frac{\\log(2/\\delta)}{2 n}}\\ .\n\\tag{6}\\]\nYou may recall that this is consistent with the central limit theorem, which states that the sampling distribution of sample means is \\(\\sigma_{\\bar{X}} = \\sigma / \\sqrt{n}\\)."
  },
  {
    "objectID": "posts/1-probability.html#multi-armed-bandits",
    "href": "posts/1-probability.html#multi-armed-bandits",
    "title": "Probability theory and random variables",
    "section": "Multi-armed bandits",
    "text": "Multi-armed bandits\nI first encountered these through a graduate course I took with Dr.¬†Bo Ji (2022); the algorithms and analyses below (as well as the concentration bounds discussion above) are all taken from lecture notes from that class, although they are originally based on a book by Slivkins (2022).\n\n\n\nImage credit: Wikipedia.\n\n\nThe image above shows a row of slot machines. They are otherwise known as one-armed bandits, as they steal money from you and have one arm you must pull to spin the wheel (although in the ones seen above it seems to have been replaced by buttons).\nGeneralizing a bit, let‚Äôs imagine that we instead have \\(K\\) such arms. Maybe they belong to the same, now much more tricky bandit. Or, maybe they model the possibility of playing different games in the same casino. In any case, at every time point \\(t\\) we must choose one such action \\(a_t \\in [K]\\) to play, only learning information about its underlying distribution through the rewards we receive.\nThere are various possible types of the bandit setting, but here we will only consider stochastic (as opposed to adversarial) losses and multi-armed (as opposed to linear or Gaussian process) bandits.\n\n\n\\begin{algorithm} \\caption{Stochastic MAB (Framework)} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Choose an action $a_t \\in [K]$. \\State Suffer loss $z_t[a_t]$ and also only observe $z_t[a_t]$. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nHere, \\(z_t\\) is a random vector sampled from the unknown distributions, i.e., \\(z_t[a] \\sim \\mathcal{D}_a\\). The defining feature separating this problem from Online Convex Optimization is being only provided one value \\(z_t[a_t]\\) corresponding to the played arm \\(a_t\\), a.k.a., bandit feedback.\nOur goal in this setting is to design an algorithm fitting this framework that minimizes the following quantity, known as regret \\[\n\\mathcal{R}_T = \\sum_{t=1}^T z_t[a_t] - \\min_i \\sum_{t=1}^T z_t[i]\\ .\n\\tag{7}\\]\nSuch a definition makes regret a useful metric in many situations, even when experiencing persistent penalties or unreasonable adversaries. In particular, it evaluates performance against the best possible fixed action in hindsight - a value that will compensate for the above scenarios. Note that the definition given in Equation¬†7 makes \\(\\mathcal{R}_T\\) a random variable, meaning that in practice we aim to bound \\(\\mathbb{E}[\\mathcal{R}_T]\\), the expected regret.2\nWe may use the terms loss and reward interchangeably, as they can be thought of as negatives of each other. This is again owed to the definition of regret in Equation¬†7, by which minimizing losses is the same as maximizing rewards.\n\nExplore-then-exploit\nA central issue in the multi-armed bandit problem is that of exploration versus exploitation - whether to prioritize investigating the yet unknown, or to take advantage of information already learned. It is not a coincidence this problem appears both here and in reinforcement learning - both fall within the realm of online learning (when decisions and information are processed sequentially), the only thing separating them is the addition of states.\nWe start with a simple algorithm that does the two stages separately (Ji 2022, Lecture 14).\n\n\n\\begin{algorithm} \\caption{Explore-then-exploit} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Explore phase: try each arm $N$ times. \\State Exploit phase: determine $\\tilde{a}$ with the highest average reward; then play $\\tilde{a}$ in all remaining rounds. \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWe wish to bound the probability of getting good approximations on all arms, which we start by bounding each arm individually. That is, we want to bound the deviation of its true mean \\(\\mu(a)\\) from its approximation \\(\\hat{\\mu}(a)\\) after that arm has been pulled \\(N\\) times. Let \\(\\beta_N = \\sqrt{\\frac{2 \\log T}{N}}\\), by Hoeffding‚Äôs inequality, \\[\n\\mathrm{P}(|\\hat{\\mu}(a) - \\mu(a)| \\leq \\beta_N) \\geq 1 - 2 / T^4\\ .\n\\tag{8}\\]\nWe define a good event \\(\\mathcal{E}\\) as the above being true for all arms, meaning that by the union bound \\[\n\\mathrm{P}(\\mathcal{E}) = 1 - \\mathrm{P}(\\bar{\\mathcal{E}}) \\geq\n1 - \\sum_{a=1}^K \\mathrm{P}(|\\bar{\\mu}(a) - \\mu(a)| \\geq \\beta_N) \\geq 1 - 2 K / T^4\\ .\n\\tag{9}\\]\nEven if this good event occurs, we may still choose a suboptimal arm \\(\\bar{a}\\) in the exploit phase. In that case, the estimate errors for both \\(\\bar{a}\\) and the optimal arm \\(a^*\\) are bounded by \\(\\beta_N\\). As such, \\(\\mu(a^*) - \\mu(\\bar{a}) \\leq 2 \\beta_N\\). Adding with a max regret of \\(1\\) for \\(KN\\) round of exploration, the total incurred regret in the good case is at most \\[\n\\mathcal{R}_T \\leq K N + (T - KN) \\cdot 2 \\beta_N \\leq KN + 2 T \\beta_N = KN + 2 T \\sqrt{\\frac{2 \\log T}{N}}\\ .\n\\tag{10}\\]\nThis also lets us derive a formula for \\(N\\) which we do by setting the resulting terms equal. In particular, \\(N = 2 \\sqrt[3]{T^2 \\log T / K^2}\\), which leads to a good case regret of \\(\\mathrm{O}(T^{2/3} (K \\log T)^{1/3})\\). The bad event happens with probability at most \\(2 K / T^4\\), which when multiplied by a max per-round regret of \\(1\\) gets absorbed by the big O.\nFor implementations of this and the following algorithms, we make a function play that returns an arm a, and a function feedback that records the resulting reward r. Both functions are supplied with other contextual parameters for convenience.\n\n\nCode for Explore-then-exploit\nfrom math import log\nfrom numpy import argmax\n\nclass ExploreExploit:\n    def __init__(self, K, T):\n        self.K = K\n        self.N = round(2 * (T**2 * log(T) / K**2)**(1/3))\n        self.totals = [0] * K\n    \n    def __str__(self):\n        return \"Explore-then-exploit\"\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if t &lt;= self.K*self.N:\n            # Play each arm N times\n            return t % self.K\n        else:\n            # Play best arm\n            return self.best\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        if t &lt;= self.K*self.N:\n            # Record reward\n            self.totals[a] += r\n        if t == self.K*self.N:\n            # Compute best arm\n            self.best = argmax(self.totals)\n\n\n\n\nEpsilon-greedy\nInstead of doing exploration all at once, we may spread it out over all rounds, making it less frequent over time (Ji 2022, Lecture 14). You‚Äôll see this strategy used quite frequently in reinforcement learning, as it is pretty simple to implement.\n\n\n\\begin{algorithm} \\caption{Epsilon-greedy} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State Toss a coin with success rate $\\epsilon_t$. \\If{success} \\State Explore: choose an arm uniformly at random. \\Else \\State Exploit: choose an arm with the highest average reward so far. \\EndIf \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWith \\(\\epsilon_t = t^{-1/3} (K \\log t)^{1/3}\\), Epsilon-greedy achieves the same regret bound, \\(\\mathrm{O}(t^{2/3} (K \\log t)^{1/3})\\). We won‚Äôt show it here for brevity, but it uses the same techniques as shown, similarly relying on Hoeffding‚Äôs inequality. Note in addition that Algorithm 3 does not rely on \\(T\\), which makes it an anytime algorithm.\n\n\nCode for Epsilon-greedy\nfrom math import log\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan\n\nclass EpsilonGreedy:\n    def __init__(self, K, _):\n        self.K = K\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"Epsilon-greedy\"\n\n    def _eps(self, t):\n        return (self.K*log(t)/t)**(1/3)\n\n    def _best(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            avg = divide(self.totals, self.counts)\n        avg[avg == inf] = -inf\n        avg[avg == nan] = -inf\n        return argmax(avg)\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if random() &lt; self._eps(t):\n            # Play random arm\n            return randrange(self.K)\n        else:\n            # Play best arm\n            return self._best()\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSuccessive elimination\nWe can further incorporate the concept of means and confidence bounds into the algorithm by considering their values at any \\(t \\in [T]\\). In particular, \\[\n\\mathrm{P} \\left( |\\hat{\\mu}_t(a) - \\mu(a)| \\leq \\beta_t(a) \\right) \\geq \\sqrt{\\frac{\\log (2 K T / \\delta)}{2 N_t (a)}}\\ .\n\\tag{11}\\]\nThis feels very similar to Hoeffding‚Äôs inequality, but actually requires a version generalized to martingales, known as the Azuma-Hoeffding inequality.\nFor convenience, we define (Ji 2022, Lecture 14) \\[\n\\begin{align}\n    \\mathrm{UCB}_t(a) &= \\hat{\\mu}_t(a) + \\beta_t(a)\\ ,\\\\\n    \\mathrm{LCB}_t(a) &= \\hat{\\mu}_t(a) - \\beta_t(a)\\ .\n\\end{align}\n\\tag{12}\\]\n\n\n\\begin{algorithm} \\caption{Successive Elimination} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\State Initialize active arm set $\\mathcal{A}_1 = [K]$. \\For{$p = 1, 2, \\ldots$} \\State Play each arm in $\\mathcal{A}_p$ once. \\State Let $t$ be the time at the end of the current phase $p$. \\State $\\mathcal{A}_{p+1} = \\{ a \\in \\mathcal{A}_p : \\mathrm{UCB}_t (a) \\geq \\max_{a' \\in \\mathcal{A}_p} \\mathrm{LCB}_t (a) \\}$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nWe define the good event as before, and assume that it holds. If we at some point eliminate \\(a^*\\), it means \\(\\mathrm{UCB}_t(a^*) &lt; \\mathrm{LCB}_t(a')\\) for some other arm \\(a'\\), implying \\(\\mu(a^*) &lt; \\mu(a')\\) and thus a contradiction. Then, assume there is some arm \\(a \\in \\mathcal{A}_{p+1}\\) during phase \\(p\\) ending at time \\(t\\) such that \\(\\mu(a^*) - \\mu(a) &gt; 4 \\beta_t(a)\\). Then, \\[\n\\mathrm{LCB}_t(a) \\leq \\mu(a) + 2 \\beta_t(a) &lt; \\mu(a^*) - 2 \\beta_t(a^*) \\leq \\mathrm{UCB}_t(a^*)\\ ,\n\\tag{13}\\] implying that this arm should have been eliminated. Thus, the regret contributed by arm \\(a\\) is \\[\n\\mathcal{R}_{a,t} \\leq 4 N_t(a) \\sqrt{\\frac{\\log(2 K T / \\delta)}{2 N_t(a)}} = \\mathrm{O}(\\sqrt{N_t(a) \\log(K T / \\delta)})\\ .\n\\tag{14}\\]\nUsing the Cauchy-Schwarz inequality, \\(\\sum_a \\sqrt{N_t(a)} \\leq \\sqrt{\\sum_a N_t (a) \\cdot \\sum_a 1} = \\sqrt{t K}\\), meaning \\[\n\\mathcal{R}_t = \\sum_{a \\in [K]} \\mathcal{R}_{a,t} = \\mathrm{O}(\\sqrt{\\log(K T / \\delta)}) \\cdot \\sum_a \\sqrt{N_t(a)} \\leq \\mathrm{O}(\\sqrt{K t \\log (K T / \\delta)})\\ .\n\\tag{15}\\]\nWe thus have that \\(\\mathcal{R}_T = \\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\).\nNote that the above bounds have a configurable value \\(\\delta \\in (0, 1]\\), which makes them take effect with probability at least \\(1 - \\delta\\). In practice, we need to set delta to a ridiculously high value for any arms to actually become eliminated within a reasonable amount of time.\n\n\nCode for Successive elimination\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass SuccessiveElimination:\n    def __init__(self, K, T, delta=1500):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n        self.A = list(range(K))\n        self.idx = 0\n    \n    def __str__(self):\n        return \"Successive elimination\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        if self.idx == len(self.A):\n            # Update arm set A\n            u, b = self._mu(), self._beta()\n            lcb, ucb = u-b, u+b\n            self.A = [a for a in self.A if ucb[a] &gt;= np.max(lcb)]\n            self.idx = 0\n        # Play each arm in A\n        a = self.A[self.idx]\n        self.idx += 1\n        return a\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nUCB\nSince the event of eliminating an arm may be quite rare, we may instead just sample the arm with the highest \\(\\mathrm{UCB}_t(a)\\). This strategy demonstrates a principle known as optimism in the face of uncertainty (Ji 2022, Lecture 15) - that is, both arms that have high sample mean and arms with fewer samples will have a larger UCB and thus more likely to get chosen.\n\n\n\\begin{algorithm} \\caption{UCB} \\begin{algorithmic} \\Require{$K$ and $T$ (both known), unknown reward distributions $\\mathcal{D}_a$.} \\For{$t = 1, 2, \\ldots$} \\State $a_t = \\arg\\max_{a \\in [K]} \\mathrm{UCB}_t (a)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\nSimilar to before, \\(\\mu(a^*) - \\mu(a_t) \\leq 2 \\beta_t (a_t)\\) in the good event. We thus have \\[\n\\mathcal{R}_T = \\sum_{t=1}^T r_t \\leq c \\sqrt{\\log (2 K T / \\delta)} \\sum_{t=1}^T \\sqrt{\\frac{1}{N_t(a_t)}}\\ ,\n\\tag{16}\\]\nwhere the latter summation is bounded by \\[\n\\sum_{t=1}^T \\sqrt{\\frac{1}{N_t(a_t)}} = \\sum_{a=1}^K \\sum_{m=1}^{N_T(a)} \\sqrt{1/m} \\leq c' \\sum_{a=1}^K \\sqrt{N_T(a)} = \\mathrm{O}(\\sqrt{K T})\\ .\n\\tag{17}\\]\nThus, the regret of UCB is \\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\).\n\n\nCode for UCB\nfrom random import random, randrange\nfrom numpy import argmax, divide, errstate, inf, nan, log, sqrt\nimport numpy as np\n\nclass UCB:\n    def __init__(self, K, T, delta=1):\n        self.num = log(2 * K * T / delta) / 2\n        assert self.num &gt; 0, \"delta too large\"\n        self.totals = [0] * K\n        self.counts = [0] * K\n    \n    def __str__(self):\n        return \"UCB\"\n\n    def _mu(self):\n        with errstate(divide='ignore', invalid='ignore'):\n            mu = divide(self.totals, self.counts)\n        mu[mu == inf] = 0\n        mu[mu == nan] = 0\n        return mu\n\n    def _beta(self):\n        with errstate(divide='ignore'):\n            beta = sqrt(divide(self.num, self.counts))\n        return beta\n\n    def play(self, t):\n        '''Choose an arm to play.'''\n        ucb = self._mu() + self._beta()\n        return argmax(ucb)\n    \n    def feedback(self, t, a, r):\n        '''Receive a reward.'''\n        self.totals[a] += r\n        self.counts[a] += 1\n\n\n\n\nSummary\nHere is a compilation of the derived bounds so far:\n\nSummary of theoretic regret bounds.\n\n\nAlgorithm\nRegret\nAnytime\n\n\n\n\nExplore-then-exploit\n\\(\\mathrm{O}(T^{2/3} (K \\log T)^{1/3})\\)\n‚ùå\n\n\nEpsilon-greedy\n\\(\\mathrm{O}(t^{2/3} (K \\log t)^{1/3})\\)\n‚úÖ\n\n\nSuccessive elimination\n\\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\)\n‚ùå\n\n\nUCB\n\\(\\mathrm{O}(\\sqrt{K T \\log (K T / \\delta)})\\)\n‚ùå\n\n\n\nOn a final note, the above bounds are all problem-independent. We can instead derive problem-dependent bounds, such as \\[\n\\mathcal{R}_T = \\sum_{a : \\Delta(a) &gt; 0} \\mathrm{O} \\left( \\frac{\\log(K T / \\delta)}{\\delta(a)} \\right)\\ ,\n\\tag{18}\\]\nfor UCB, where \\(\\Delta(a) \\coloneqq \\mu(a^*) - \\mu(a)\\).\n\n\nSimulation\nNow, it‚Äôs time to put the algorithms to the test!\n\nTechnical note\nBut first, a small note on the simulation details. As the rewards (and thus performance) are highly stochastic, we will need to run many trials to get an accurate result. Even when results are recorded once every F steps, we still run into situations where the times for storing and churning data may compete with those of running the algorithms. We have several possible approaches:\n\nStore them all: pre-allocate a huge array, and process it once afterwards. This understandably causes slowdowns.\nAggregate last: i.e., iterate over algorithms and timesteps first, then trials. This may seem like a perfect solution at first, but it runs into a caveat of the algorithms storing internal parameters. That is, algorithms from some runs will have more information about certain arms than others, so we can‚Äôt just re-sample the same instance many times. The way to address this would involve storing an array of algorithms proportional to the number of trials, which is again inefficient.\nOnline aggregation: iterate over trials first, and store a statistic for each algorithm/timestep. The way we avoid having a separate dimension for trials is via an approximation algorithm that aggregates statistics as they come in - Welford‚Äôs online algorithm.\n\nWe will use the latter approach, which is what lets us compute 1000 samples for each data point.\n\n\nSeed random number generators\nfrom random import seed as py_seed\nfrom numpy.random import seed as np_seed\npy_seed(5805)\nnp_seed(5805)\n\n\nIn particular, we will simulate having to choose between \\(K=2\\) arms:\n\nA uniform distribution on \\([0,1]\\), i.e., mean \\(0.5\\) and standard deviation \\(\\sqrt{1/12} \\approx 0.29\\).\nA normal distribution with mean \\(0.6\\) and standard deviation \\(0.2\\).\n\nThe above statistics indicate that arm 2 is more optimal, but arm 1 has a wider distribution of rewards.\n\n\nSimulation code\nfrom random import random\nfrom numpy.random import normal\nfrom numpy import zeros, sqrt\n\n# Simulation parameters\nT = 1000    # Number of rounds\nF = 10      # Logging frequency\nN = 1000    # Number of trials\n\n# Arms to explore\nmeans = [0.5, 0.6]\nbest = max(means)\narms = [random, lambda: normal(means[1], 0.2, 1).item()]\nK = len(arms)\n\n# Algorithms to use\ncs = [ExploreExploit, EpsilonGreedy, SuccessiveElimination, UCB]\nA = len(cs)\n\n# Evaluate algorithms\nx = zeros((T//F+1, A))\ns = zeros((T//F+1, A))\nfor n in range(1, N+1):\n    for i, c in enumerate(cs):\n        alg = c(K, T)\n        rgt = 0\n        for t in range(1, T+1):\n            a = alg.play(t)\n            r = arms[a]()\n            alg.feedback(t, a, r)\n            rgt += best - r\n            if t % F == 0:\n                # Welford's online algorithm\n                xn1 = x[t//F, i]\n                x[t//F, i] = ((n-1)*xn1 + rgt) / n\n                s[t//F, i] += (rgt - xn1) * (rgt - x[t//F, i])\ns = 0.2*sqrt(s / (N-1))\n\n\nWe now plot the mean regret \\(\\mathcal{R}_t\\) of each algorithm, as well as the \\(\\pm 0.2\\) sample standard deviation margins.\n\n\nCode for plotting regret\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(1, T+2, F)\nplt.plot(t, x)\nfor i in range(A):\n    plt.fill_between(t, x[:,i]-s[:,i], x[:,i]+s[:,i], alpha=.15)\nplt.xlabel(\"Timestep\")\nplt.ylabel(\"Regret\")\nplt.legend([c(K, T) for c in cs]);\n\n\n\n\n\nFigure¬†1: Performance of MAB algorithms\n\n\n\n\nThe above performance roughly matches what we should expect. Most notably, Explore-then-exploit accumulates linear regret until a certain time, where it sharply switches to exploitation and plateus. The other algorithms gently bend down towards it instead, allowing them to achieve lower final regrets.\nNote that the above shouldn‚Äôt be used as a definitive guide for how the algorithms compare, as it‚Äôs specific to the problem instance and algorithm parameters. In particular, the performance of Successive elimination is highly dependent on its artificial parameter \\(\\delta = 1500\\), as it determines how soon the trend starts bending toward a plateu. Still, it may give an indication as to why the UCB algorithm and its variants are so common in practice."
  },
  {
    "objectID": "posts/1-probability.html#footnotes",
    "href": "posts/1-probability.html#footnotes",
    "title": "Probability theory and random variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese \\(X_i\\) can either be thought of as fixed values or i.i.d. random variables. In coming examples, we will use the latter.‚Ü©Ô∏é\nIn practice, it often ends up more convenient to bound the pseudo-regret \\(\\bar{\\mathcal{R}}_T\\), which swaps the order of \\(\\mathbb{E}\\) and \\(\\min\\), turning it into a \\(\\max\\). By Jensen‚Äôs inequality, \\(\\mathbb{E}[\\mathcal{R}_T] \\geq \\bar{\\mathcal{R}}_T\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alice Didenkova",
    "section": "",
    "text": "A graduate student at Virginia Tech."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alice Didenkova",
    "section": "Education",
    "text": "Education\nVirginia Tech | Blacksburg, VA\nPh.D.¬†Candidate in Computer Science | 2022 - present\nVirginia Tech | Blacksburg, VA\nB.S. in Computer Science, Mathematics | 2019 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Alice Didenkova",
    "section": "Experience",
    "text": "Experience\nFujitsu Research of America | Research Intern in Quantum Computing | Summer 2023\nCS 3214: Computer Systems | Teaching Assistant | 2021 - present"
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Alice Didenkova",
    "section": "Topics",
    "text": "Topics\nMy interests and current research topics include:\n\n‚öõÔ∏è Quantum computing\nü§ñ Reinforcement learning\n\nDisclaimer: this blog has been created to fulfill the Final Project requirements of CS 5805: Machine Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability theory and random variables\n\n\nConcentration bounds and multi-armed bandits\n\n\n\n\nProbability theory\n\n\nRandom variables\n\n\nConcentration inequalities\n\n\nMulti-armed bandits\n\n\nOnline learning\n\n\n\n\nIn which we introduce random variables and describe the theory behind bounding them via concentration inequalities. We then introduce several multi-armed bandit algorithms to see how such inequalities may be used to evaluate their performance.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\nSparse subspace clustering\n\n\n\n\nClustering\n\n\nUnsupervised learning\n\n\nEigenvalues\n\n\nConvex optimization\n\n\n\n\nIn which we overview common tasks and models for clustering. Includes an exploration of spectral clustering and sparse subspace clustering, and an implementation of the latter.\n\n\n\n\n\n\nDec 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nRegression\n\n\nSupervised learning\n\n\nLeast squares\n\n\n\n\nIn which we analyze various methods for linear and nonlinear regression. Includes several appearances from nonparametric regression.\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nClassification\n\n\nSupervised learning\n\n\nLearning theory\n\n\nNaive Bayes\n\n\nNeural network\n\n\n\n\nIn which we overview classification, applying it to a simple and then a more complicated dataset.\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/outlier detection\n\n\nChangepoint detection\n\n\n\n\nOutliers\n\n\nRegression\n\n\nChangepoint detection\n\n\nOnline learning\n\n\n\n\nIn which we analyze ways to detect anomalies and account for outliers. Features robust regression and changepoint detection.\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2-clustering.html",
    "href": "posts/2-clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior information about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn‚Äôt belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn‚Äôt a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here‚Äôs a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure¬†1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that‚Äôs a lot of them! If you‚Äôve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure¬†1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph‚Äôs weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation¬†1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster‚Äôs similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell‚Ä¶ sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This addresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke‚Äôs law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for instance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that‚Äôs all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure¬†2 below.\n\n\n\nFigure¬†2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definition of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure¬†2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‚ÄòX‚Äô shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # ‚ñ° Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#clustering",
    "href": "posts/2-clustering.html#clustering",
    "title": "Clustering",
    "section": "",
    "text": "Falling within the realm of unsupervised learning, this task deals with unlabeled but adequately distinct data. While both it and classification involve modeling such differences, clustering in particular starts with no prior information about data labels. In cases where partial label information is known, the task is instead considered to be semi-supervised.\nThe common goal of clustering is to separate data into clusters - groups such that data within them is similar and between them is different. What this means precisely depends on the measure of similarity used; for raw Euclidean distances it would mean that clusters are far apart literally, whereas for feature embedding spaces they would be apart semantically/conceptually.\nClustering serves a variety of purposes:\n\nUnderstanding: separating data into distinct groups may reveal patterns and highlight important features. Additionally, clustering may be viewed as a form of divide-and-conquer where the clusters may be easier to analyze in isolation.\nPreprocessing: similar to the above, the benefit of being able to analyze data separately can be extended to programmatic solutions. That is, cluster assignment can be used to split data and train models separately for each.\nAnomaly detection: the above would additionally get rid of outliers as they wouldn‚Äôt belong to any cluster. This makes clustering applicable to anomaly detection as it can identify points that are distant from all other data.\n\nClustering in itself isn‚Äôt a method but a task. A plethora of algorithms exist to perform it, thanks to various relevant considerations about the type of clustered data:\n\nShape: while we may typically think of clusters as circular-like objects, they may instead be stretched, spread out, stretched out, bent, etc. For the stretched out type, it additionally becomes important whether data exhibits linear relationships or otherwise lies in nonlinear manifolds.\nData type: the same considerations from other tasks in machine learning apply: is the data numerical or categorical, are any transforms applicable, is some of the data images/audio/text/etc?\nDimensionality: for more complex data types, preprocessing or embedding may be in order. High-dimensional data in general poses a challenge due to the curse of dimensionality, a phenomenon where regular measures of distance break down and lose meaning. In some circumstances it thus becomes crucial to use methods specifically designed around high-dimensional data.\n\n\n\nTo illustrate the former consideration particularly, we can take a look at some distinct sets of artificial data and the performances of various clustering algorithms. Here‚Äôs a pretty nice visualization of some common algorithms:\n\n\nClustering scatterplot matrix code by sklearn\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nfrom ssc import linear, SparseSubspaceClustering\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 200\nseed = 5805\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 5805\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(10.5, 6.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.01, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (linear, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    hdbscan = cluster.HDBSCAN(\n        min_samples=params[\"hdbscan_min_samples\"],\n        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n        allow_single_cluster=params[\"allow_single_cluster\"],\n    )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n    ssc = SparseSubspaceClustering(\n        n_clusters=params[\"n_clusters\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n        (\"Sparse\\nSubspace\\nClustering\", ssc),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph, subspace clustering\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Solution may be inaccurate. Try another solver,\"\n                + \" adjusting the solver settings, or solve with\"\n                + \" verbose=True for more information.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=9)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=7.5,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n\n\n\nFigure¬†1: A comparison of various clustering methods.\n\n\n\n\nWhoa, that‚Äôs a lot of them! If you‚Äôve seen this image before and are particularly observant, you may notice that I threw in an extra algorithm (rightmost column) and a dataset (second-bottom row) - more on those later.\nEach of the above algorithms is deserving of a post of its own. Here, I will just briefly focus on one, which happens to be the only showcased algorithm that properly clusters the top 5 datasets:\n\n\n\nYes, spectral as in spectrum - i.e., eigenvalues. I found out about this algorithm back when I was taking intro to linear algebra, and it really stuck with me precisely because it gives a pretty neat interpretation of eigenvalues (which at the time I had a really hard time exemplifying).\nWe start with a similarity matrix \\(S\\), where the \\(ij\\)-th value denotes the similarity of data points indexed \\(i\\) and \\(j\\). What constitutes as being similar may vary; the example in Figure¬†1 simply uses Euclidean distance. We then use \\(S\\) to construct a similarity graph, a weighted graph with weights \\(w_{i,j} = s_{i,j}\\) if they are past a certain threshold and \\(0\\) (no edge) otherwise. This graph‚Äôs weighted adjacency matrix \\(W\\) is essentially \\(S\\) with all sub-threshold values zeroed out. Assuming data from \\(k\\) sufficiently distinct clusters is ordered by cluster, the adjacency matrix may look something like \\[\nW = \\begin{bmatrix} W_1 & & & \\\\ & W_2 & & \\\\ & & \\ddots & \\\\ & & & W_k \\end{bmatrix}\\ ,\n\\tag{1}\\]\nwhich you may recognize as a block diagonal matrix.\nSo, what role do eigenvalues play here? Turns out, if \\(v\\) is an eigenvalue of \\(W_1\\), then the same \\(v\\) padded by a bunch of zeros at the bottom is an eigenvalue of \\(W\\). You can verify this via Equation¬†1; \\(W_1 v = v \\lambda\\) necessarily implies \\(W \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} v \\\\ 0 \\end{bmatrix} \\lambda\\). The same applies for eigenvectors of any cluster, they just need to placed appropriately to act on their cluster‚Äôs similarity block \\(W_i\\). As such, the eigenvalues of \\(W\\) will be grouped according to cluster, having zero values in all other locations. Finding these eigenvalues would then reveal the clusters.\nWell‚Ä¶ sort of. In reality, we still have some problems: the eigenvalues can have small/zero entries, and finding all \\(n\\) eigenvalues is just too difficult. Instead, we define \\(D\\) to be the diagonal degree matrix, its diagonal entries being the sum of the corresponding rows/columns of \\(W\\). We then obtain the graph Laplacian \\(L = D - W\\), another diagonal block matrix with an additional property that its rows/columns sum to zero. This makes it so multiplying each block \\(L_i\\) by a vector with all entries being \\(1\\) produces the zero vector, making it an eigenvector of \\(L_i\\) with a zero eigenvalue. Additionally, the only vectors satisfying this property are just scalar multiples of that identity vector. As such, the Laplacian \\(L\\) will have its zero eigenspace spanned by precisely \\(k\\) such identity vectors. This addresses both of our previous problems, making it so we only have to find \\(k\\) out of \\(n\\) eigenvectors. Using inverse iteration will obtain these eigenvectors only, and do so very efficiently. Putting the eigenvectors together as a matrix would then make its rows indicators for which cluster a particular point belongs to. In case there is still any ambiguity, the rows can be grouped together using a simpler clustering algorithm like \\(k\\)-means. For more details about spectral clustering, see Luxburg (2007).\n\n\nA pretty cool interpretation of this Laplacian comes from thinking of the nodes in the similarity graph as being connected by springs - the higher the weight between two nodes, the stronger their connecting spring. What happens if we move a single node by some amount? The nodes connected to it would be pulled in the same direction according to the strengths of the springs that connect them to the displaced node, the latter being pulled back with the sum of those strengths. Note that this force is precisely described by the (negative of) the column of \\(L\\) corresponding to the displaced node. If \\(x\\) is the vector representing the displacement of each node, the corresponding force (in the other direction) is given by \\(Lx\\). In the case where the displacement is an eigenvector of \\(L\\), we have \\[\n\\sum F = - L v = - \\lambda v\\ ,\n\\tag{2}\\]\nwhich is precisely Hooke‚Äôs law. Thus given an initial displacement \\(v\\), the graph will undergo simple harmonic motion with frequency \\(\\lambda^2\\). In other words, the eigenpairs \\((\\lambda, v)\\) of \\(L\\) actually describe the resonant frequencies \\(\\lambda^2\\) and corresponding vibration modes of its graph. It is for this reason that eigenvalues are crucial to analyzing mechanical resonance - where the graph represents connected parts of a physical object. This could be applied to, for instance, designing buildings that are resistant to strong winds and earthquakes.\nIn our case we saw that the similarity graph had some zero eigenvalues. This corresponds to zero frequency - i.e., once we displace the nodes of a single cluster, it experiences no force from the others and just stays in the same place.\n\n\n\n\nSo, that‚Äôs all well and good when similarity relates to Euclidean distance - but as noted before, this becomes less true for high-dimensional data. Instead, data may lie in linear subspaces, such as when representing the same scene illuminated from different angles, or objects moving at different speeds Elhamifar and Vidal (2013). A simplified image of what such data could look like is seen in Figure¬†2 below.\n\n\n\nFigure¬†2: Three linear subspaces. Figure by Elhamifar and Vidal (2013).\n\n\nThe point where the lines and plane intersect is the origin. You may note that this matches the mathematical definition of linear subspaces, i.e., non-empty and closed under addition and scalar multiplication. Alternatively, we may pick any number of vectors and have their span be a linear subspaces. Any point in the subspaces would then be a linear combination of such vectors, which are themselves points in the subspace.\nYou may note that the new dataset is somewhat like a flattened version of Figure¬†2, with the lines \\(\\mathcal{S}_2\\) and \\(\\mathcal{S}_3\\) being the diagonals making up the ‚ÄòX‚Äô shape, and the plane \\(\\mathcal{S}_1\\) being the remaining points in the background square.\n# The 'X'-looking dataset\n\nlinear = np.concatenate((\n        rng.rand(n_samples//2, 2),                              # ‚ñ° Square\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1),  # / Diagonal\n        rng.rand(n_samples//4, 2) * 0.05\n            + np.repeat(rng.rand(n_samples//4, 1), 2, axis=1)   # \\ Diagonal\n            * [1, -1] + [0, 1]\n    ), axis=0) - [0.5, 0.5], None\nOur goal is then to separate the two diagonals and the square."
  },
  {
    "objectID": "posts/2-clustering.html#sparse-subspace-clustering",
    "href": "posts/2-clustering.html#sparse-subspace-clustering",
    "title": "Clustering",
    "section": "Sparse Subspace Clustering",
    "text": "Sparse Subspace Clustering\nTo this end, we will analyze an algorithm based on spectral clustering, proposed by Elhamifar and Vidal (2013). This algorithm relies on the self-expressiveness property described above, i.e., that points in a subspace can be described by a linear combination of other points from the same subspace. Specifically, let \\(Y = \\begin{bmatrix} y_1 & y_2 & \\dots & y_n \\end{bmatrix}\\) be a \\(d\\)-by-\\(n\\) matrix of data. Then, with every data point \\(y_i\\) we can associate a coefficient vector \\(c_i\\) such that \\[\ny_i = Y c_i\\ ,\\qquad\nc_{ii} = 0\\ .\n\\tag{3}\\]\nAs the above has infinitely many solutions, we will additionally \\(\\text{minimize} ||c_i||_q\\). Using smaller \\(q\\) will make more coefficients zero, and for \\(q=1\\) will generally only have nonzero coefficients for points within the same subspace.1\nLetting \\(C = \\begin{bmatrix} c_1 & c_2 & \\dots & c_n \\end{bmatrix}\\), we can rewrite the problem in matrix form as \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 \\\\\n    \\text{such that} \\quad& Y = Y C\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{4}\\]\nNote that the objective is convex and the constraints affine, so this can be solved efficiently using\n\nConvex optimization\nA framework for problems with certain nice properties, convex optimization can be applied to a multitude of tasks in various fields. While I first encountered it in exactly this context of sparse subspace clustering, I now use a variant of it (semidefinite programming) all the time for quantum-related stuff.\nLet‚Äôs solve an instance of Equation¬†4 in practice to see why this matrix \\(C\\) could be useful. First, let‚Äôs construct a simplified version of the dataset shown above, containing fewer points and only the diagonals.\n\n\nGenerate a simpler dataset\nn = 20\nY = np.concatenate((\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0),  # / Diagonal\n    rng.rand(2, n//2) * 0.01\n        + np.repeat(rng.rand(1, n//2), 2, axis=0)   # \\ Diagonal\n        * [[1], [-1]] + [[0], [1]]\n), axis=1) - [[0.5], [0.5]]\n\n\nTo construct and solve the convex optimization problem, we will use the python package cvxpy. We essentially just need to declare \\(C\\) as a variable, then specify our objective and constraints. The rest is handled by the solver - everything from verifying that the problem is convex to returning its solution.\n\n\nSolve and visualize a convex optimization problem\nfrom cvxpy import Variable, Minimize, norm, diag, Problem, ECOS\nimport plotly.express as px\n\n# Variable matrix\nC = Variable((n, n))\n\n# Minimize ||C||_1\nobjective = Minimize(norm(C,1))\n\n# Subject to Y = YC, diag(C) = 0\nconstraints = [Y == Y @ C, diag(C) == 0]\nprob = Problem(objective, constraints)\nprob.solve(solver=ECOS)\n\n# Normalize coefficient matrix\nC = C.value / np.max(np.abs(C.value), axis=0)\npx.imshow(C)\n\n\n\n\n                                                \nFigure¬†3: A heatmap of the obtained coefficient matrix \\(C\\).\n\n\n\n?@fig-C above shows what the (normalized) solution looks like - you may note that it‚Äôs a block diagonal matrix much like Equation¬†1. This is due to the self-expressiveness property; points from a subspace will only have nonzero coefficients corresponding to other points from the same subspace. As such, we may essentially use this \\(C\\) matrix as similarity for (spectral clustering)[#spectral-clustering].\nMore specifically, you may note that there are two rows with quite large coefficients whereas the others are quite small. This comes from minimizing the norm of \\(C\\), leading to us to prioritize points with larger distances from the origin. In the case of one-dimensional subspaces like this, we end up picking the farthest point from the origin and expressing the remaining points as scalar multiples of it. To better approximate the block structures and make the similarity graph undirected, we thus symmetrize the matrix via \\(W = |C| + |C|^\\top\\).\n\n\nImplementation\nAs one last detail, Elhamifar and Vidal (2013) proposes a modified version of Equation¬†4 that deals with noise \\[\n\\begin{align}\n    \\text{minimize} \\quad& ||C||_1 + \\lambda_e ||E||_1 + \\frac{\\lambda_z}{2} ||Z||_F^2 \\\\\n    \\text{such that} \\quad& Y = Y C + E + Z\\ ,\\\\\n    & \\mathrm{diag}(C) = 0\\ .\n\\end{align}\n\\tag{5}\\]\nThe \\(E\\) matrix is meant to account for outliers and the \\(Z\\) matrix for other noise. The reference material suggests setting \\(\\lambda_e = \\alpha_e / \\mu_e\\) and \\(\\lambda_e = \\alpha_e / \\mu_e\\) with \\[\n\\mu_e = \\min_i \\max_{j \\neq i} ||y_j||_1\\ ,\\qquad\n\\mu_z = \\min_i \\max_{j \\neq i} |y_i^\\top y_j|\\ .\n\\tag{6}\\]\nPutting everything together, the algorithm looks like this:\ndef fit(self, Y):\n    Y = Y.T\n    d, n = Y.shape\n    assert d &lt; n\n\n    ## 1. Solve sparse optimization problem\n\n    # Starting form, equation (5)\n    C = Variable((n, n))\n    objective = norm(C,1)\n    constraint = Y @ C\n\n    # Account for outliers\n    if self.use_E:\n        mu_e = np.partition(np.sum(np.abs(Y), axis=0), -2)[-2]\n        l_e = 20 / mu_e\n        E = Variable((d, n))\n        objective += l_e * norm(E,1)\n        constraint += E\n\n    # Account for noise\n    if self.use_Z:\n        G = np.abs(Y.T @ Y)\n        mu_z = np.min(np.max(G - np.diag(np.diag(G)), axis=1))\n        l_z = 800 / mu_z\n        Z = Variable((d, n))\n        objective += l_z/2 * norm(Z,\"fro\")**2\n        constraint += Z\n\n    constraints = [Y == constraint, diag(C) == 0]\n    prob = Problem(Minimize(objective), constraints)\n    try:\n        prob.solve(solver=ECOS)\n\n        ## 2. Normalize the columns of C\n        C = C.value / np.max(np.abs(C.value), axis=0)\n\n        ## 3. Form the weights of a similarity graph\n        W = np.abs(C)\n        W = W + W.T\n\n        ## 4. Apply spectral clustering on the graph\n        self.labels_ = SpectralClustering(\n                n_clusters=self.n_clusters,\n                affinity='precomputed'\n            ).fit_predict(W)\n    except SolverError:\n        self.labels_ = np.zeros(n)"
  },
  {
    "objectID": "posts/2-clustering.html#footnotes",
    "href": "posts/2-clustering.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we could also solve for \\(q=0\\), but this is an \\(\\mathsf{NP}\\)-hard problem.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/4-classification.html",
    "href": "posts/4-classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a supervised learning method, much like regression. Unlike regression, it predicts features not continuous, but categorical and unordered. Such features then denote the class that each point belongs to - not unlike assigning a cluster. Indeed, both classification and clustering work with data that is typically sufficiently distinct, that they must then learn to separate into groups. The crucial difference is that classification is given the labels of the training data, which it then must generalize to the testing set.\nVarious considerations and regimes for classification exist:"
  },
  {
    "objectID": "posts/4-classification.html#learning-theory",
    "href": "posts/4-classification.html#learning-theory",
    "title": "Classification",
    "section": "Learning theory",
    "text": "Learning theory\nBefore we dive into specific algorithms and their applications, it would be useful to review the mathematical theory behind clustering (and machine learning more broadly). This is a fairly mature and encompassing theory, being an umbrella term for many important concepts such as VC theory, online learning, and PAC learning.\nSuppose we have some unknown target function \\(f^\\dagger: \\mathcal{X} \\to \\mathcal{Y}\\) that maps data to labels. We wish to model this function, but the only information we have about it is how it acts on the data. In addition, we only have a finite amount of training examples \\(D = \\{(x_1,y_1), \\ldots (x_n,y_n)\\} \\sim \\mathcal{D}\\). We wish to approximate \\(f^\\dagger\\) via some other function \\(\\hat{f}\\) that not only maps each \\(x_i \\mapsto y_i\\), but also generalizes to doing this for any \\((x,y) \\sim \\mathcal{D}\\). The approximation \\(\\hat{f}\\) will instead map \\(x\\) to some \\(\\hat{y}\\), and to measure the quality of this output, we will choose a loss function \\(L(\\hat{y},y)\\) of interest (often, the indicator function). More formally, we can evaluate an approximation \\(f\\) via its risk, defined as \\[\nR(f) = \\mathbb{E}[L(f(x), y)] = \\int L(f(x), y) \\mathrm{dP}(x,y)\\ .\n\\tag{1}\\]\nThis metric choice may seem a bit odd, but necessary since sampling \\((x,y)\\) from \\(\\mathcal{D}\\) means they are random variables. In addition, we can‚Äôt even evaluate the above quantity since we don‚Äôt know \\(\\mathcal{D}\\) in the first place.\nSo, what to do? Instead, we just evaluate on the available data to obtain the empirical risk of \\(f\\), \\[\nR_\\text{emp}(f) = \\frac{1}{n} \\sum_{i=1}^n L(f(x_i), y_i)\\ .\n\\tag{2}\\]\nOutside of learning theory, risk and empirical risk may be referred to as testing and training accuracy.\nThe next question is how to find our approximation \\(\\hat{f}\\). To do this, we need to start with picking a hypothesis class \\(\\mathcal{H}\\), representing all possible models (and parameters) we could potentially end up with. We then compute \\[\n\\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} R_\\text{emp}(f)\\ ,\n\\tag{3}\\]\nand hope that it is close to \\[\nf^* = \\arg\\min_{f \\in \\mathcal{H}} R(f)\\ .\n\\tag{4}\\]\nSo, at this point we have already accumulated a couple potential sources of error. The following picture helps to illustrate them.\n\n\n\nImage credit: Han Bao.\n\n\nWhen restricting ourselves to \\(\\mathcal{H}\\), we demoted the best attainable function from \\(f^\\dagger\\) to \\(f^*\\) resulting in an approximation error. Then, since we only have a limited-size dataset, the best possible approximation to \\(f^*\\) we can get is \\(\\hat{f}\\), which incurs an estimation error.\nSo, what should we do to minimize these errors? The quality of the approximation primarily depends on the chosen hypothesis class - making it include more complex models decreases the error. As for estimation, there are various bounds that relate it to parameters; though, the common theme is that generalization is improved with a larger \\(|D| = n\\) and a smaller \\(|\\mathcal{H}|\\). However, the latter condition seems almost directly contradictory to the one we need for better approximations, i.e., more complex models. The following graphic illustrates this tradeoff (occasionally also referred to as the underfitting/overfitting or bias/variance tradeoff)\n\n\n\nTradeoff of estimation and approximation errors.\n\n\nSo, is all lost? Not exactly; the conditions aren‚Äôt directly contradictory - we could minimize both by finding an \\(\\mathcal{H}\\) that contains as few models as possible, but the ones it contains are accurate at modeling the desired data. This is a task easier said than done, as we need to carefully discard all unnecessary models, while making sure the important ones are retained. The search for such a hypothesis class \\(\\mathcal{H}\\) is perhaps what drives the creation of many various types of machine learning algorithms."
  },
  {
    "objectID": "posts/4-classification.html#simple-dataset",
    "href": "posts/4-classification.html#simple-dataset",
    "title": "Classification",
    "section": "Simple dataset",
    "text": "Simple dataset\nWe start our evaluation by analyzing one of the algorithms featured in Figure¬†1 (second column from the right), Naive Bayes. We can see that it performs fairly well on the artificial data, separating it sufficiently but also generalizing smoothly to outside of the provided domain. Let‚Äôs see how it fares against data with a couple more dimensions.\n\nNaive Bayes\nBut first, a small introduction of the algorithm itself (mostly based on its Wikipedia article). As evident by its name, it relies on Bayes‚Äô theorem \\[\n\\mathrm{P}(k | x) = \\frac{\\mathrm{P}(x | k) \\mathrm{P}(k)}{\\mathrm{P}(x)}\\ ,\n\\tag{5}\\]\nwhere \\(k\\) is the event of the data \\(x = x_1, \\ldots x_n\\) belonging to class \\(k\\). Naturally, the quantity on the left of Equation¬†5 is exactly what would let us perform classification, so we use the quantity on the right (ignoring the denominator as it doesn‚Äôt depend on \\(k\\)) to compute it. The numerator is equal to \\(\\mathrm{P}(x, k)\\), which we may by chain rule write as \\[\n\\mathrm{P}(x_1, \\ldots, x_n, k)\n= \\mathrm{P}(x_1 | x_2, \\ldots x_n, k) \\mathrm{P}(x_2 | x_3, \\ldots x_n, k) \\dots \\mathrm{P}(x_n, k) \\mathrm{P}(k)\\ .\n\\tag{6}\\]\nTo go further, we need an assumption - the conditional independence (a.k.a. naive Bayes) assumption. Specifically, we have that \\(\\mathrm{P}(x_i | x_{i+1}, \\ldots, x_n, k) = \\mathrm{P} (x_i | k)\\), and thus \\[\n\\mathrm{P}(k | x) \\propto \\mathrm{P}(k, x) = \\mathrm{P}(k) \\prod_{i=1}^n \\mathrm{P}(x_i | k)\\ .\n\\tag{7}\\]\n\n\nIris\nI don‚Äôt think this dataset really needs an introduction, you‚Äôve almost certainly seen it before. In any case, it has four features that describe the length and width of flower petals and sepals, as seen below.\n\n\n\nImage credit: Sebastian Raschka.\n\n\nThe question is; could you tell which of the following three flowers you‚Äôre looking at?\n\n\n\nImage credit: Gaurav Chauhan.\n\n\nExcept, you wouldn‚Äôt be actually looking at the flowers but instead just those four numbers. It may instead be easier to see the difference if the values are plotted laterally. Below, petal length is denoted by size and petal width by transparency.\n\n\n2D scatterplot of iris\nimport plotly.express as px\nfrom sklearn.preprocessing import MinMaxScaler\n\niris = px.data.iris().drop('species_id', axis=1)\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", marginal_y=\"violin\", marginal_x=\"violin\",\n    template=\"simple_white\", hover_data=['petal_width'])\nop = MinMaxScaler(feature_range=(0.5, 1.0)).fit_transform(\n    iris['petal_width'].values[:, None])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfig.show();\n\n\n\n                                                \n\n\nOr, perhaps in a 3D plot that shows all four features (petal length is still marker size).\n\n\n3D scatterplot of iris\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", template=\"simple_white\")\nfig.show();\n\n\n\n                                                \n\n\nI don‚Äôt know about you, but some of the points between versicolor and virginia still seem pretty overlapping. Let‚Äôs see how the algorithm compares!\n\n\nEvaluation\nLucky for us, Naive Bayes is a common algorithm with many implementations. We‚Äôll be using one by sklearn.\n\n\nNaive bayes classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score\n\n# Fit and predict\nX, y = iris.drop('species', axis=1), iris['species']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=5805)\nnb = GaussianNB().fit(X_train, y_train)\niris['predict'] = nb.predict(X)\ny_pred = nb.predict(X_test)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.9467\nPrecision: 0.9444\n\n\n\n\nCode for plotting the confusion matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nnames = iris['species'].unique()\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (4,3))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\nFrom the above, we can gauge the performance as quite good - the algorithm only mislabels \\(4\\) points out of a testing set of size \\(75\\), while being given a training set of only size \\(75\\).\nLet‚Äôs see where it made mistakes. Predictions are denoted by outline colors, points where the inner color doesn‚Äôt match are mislabeled.\n\n\n2D scatterplot of iris & predictions\nimport numpy as np\n\nfix = {\"circle\": \"#1F77B4\", \"diamond\": \"#FF7F0E\", \"square\": \"#2CA02C\"}\nfig = px.scatter(iris, x=\"sepal_width\", y=\"sepal_length\", size=\"petal_length\",\n    color=\"species\", symbol=\"predict\",\n    template=\"simple_white\", hover_data=['petal_width'])\nfig.update_traces(marker=dict(opacity=op), selector=dict(mode='markers'))\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(width=3, color=color),\n                      symbol=\"circle\"),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();\n\n\n\n                                                \n\n\nAs expected, only a few points are mislabeled. There were a tiny bit more than in the confusion matrix, as the scatterplot includes both the training and testing sets.\nWe can also display this on the 3D plot, but it unfortunately has a bug where the outline thickness cannot be increased and is barely visible. To compensate, predicted classes are additionally denoted by the marker shape.\n\n\n3D scatterplot of iris & predictions\nimport plotly.express as px\n\nfig = px.scatter_3d(iris, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_width\",\n    size=\"petal_length\", color=\"species\", symbol=\"predict\",\n    template=\"simple_white\")\nfor symbol, color in fix.items():\n    fig.update_traces(marker=dict(line=dict(color=color)),\n                      selector=dict(mode='markers', marker_symbol=symbol))\nfig.show();\n\n\n\n                                                \n\n\nThis makes the mislabeled points a bit easier to identify, revealing that there are \\(6\\) in total."
  },
  {
    "objectID": "posts/4-classification.html#complex-dataset",
    "href": "posts/4-classification.html#complex-dataset",
    "title": "Classification",
    "section": "Complex dataset",
    "text": "Complex dataset\nThat seems to have been too easy. Let‚Äôs try something harder.\n\nMNIST\nOnce again, you‚Äôve probably seen this one before. The data consists of hand-drawn digits, which we must then classify.\n\n\n\nImage credit: Orhan G. Yal√ßƒ±n.\n\n\n\n\nNeural network\nDue to the high-dimensionality of the data, we now need to look beyond classical machine learning methods. One approach that has gained widespread popularity over the last decade is deep learning, i.e., models with many layers to model complex semantics.\nSuch models are commonly neural networks - architectures supposedly inspired by neurons in a brain. Each neuron takes inputs from several others, aggregates them (plus a bias), then passes through an activation function and onto the next layer. The nonlinearity of the activation step in particular makes it possible for complex behaviors to arise from this structure. Another important reason for these networks being so widespread is that the neurons are neatly arranged in layers, making their associated operations efficiently computable via matrix multiplication - an operation with abundant hardware dedicated to it thanks to computer graphics.\nIn the following example, we‚Äôll be using a special type of neural network - a convolutional neural net (CNN) - specifically designed to process image inputs. Its layers replace the fully-connected structure by the convolution of a single kernel of weights. This makes the patterns learned by the kernels applicable to many parts of the image, reduces the number of model parameters (lower \\(|\\mathcal{H}|\\) - good!) while being able to model data effectively (lower approximation error - good!), and introducing spatial locality into the architecture.\nFor this example, we‚Äôll be mostly following this pytorch tutorial. First, load and normalize the dataset. Note that pytorch does this through a class called DataLoader, which has the benefit of avoiding storing the entire dataset in memory.\n\n\nImport pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n\nUsing device: cpu\n\n\n\n\nLoad and preprocess data\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5), (0.5))\n    ])\nbatch_size = 4\n\ntrainset = MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nnames = list(range(10))\n\n\nNow, define the actual neural network. I‚Äôve condensed1 it quite a bit from the tutorial, keeping the two convolutional layers but removing some dense layers at the end.\n\n\nCreate a neural network\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 3, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(3, 6, 5)\n        self.fc1 = nn.Linear(6 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        return x\n\nmodel = NeuralNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n\n\n\nEvaluation\nAnd now, let‚Äôs actually train and evaluate it. If you‚Äôre following along, this step can take quite a while‚Ä¶\n\n\nTrain a neural network\n# Do one pass over the dataset\nfor i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n\n    # Resent the gradients\n    optimizer.zero_grad()\n\n    # Forward/backward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n\n# Evaluate network\ny_test, y_pred = [], []\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        y_test.extend(labels)\n        y_pred.extend(pred)\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\\n\"\n      f\"Precision: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n\n\nAccuracy:  0.8987\nPrecision: 0.9056\n\n\n\n\nCode for plotting the confusion matrix\nconf = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(conf, index=names, columns=names)\nplt.figure(figsize = (8.5,7))\nsn.heatmap(df_cm, annot=True, fmt='d');\n\n\n\n\n\n\n\n\n\nYay, it‚Äôs finished! The performance above is not too shabby, although bigger and better-trained models should attain a few more percent of accuracy.\nThe adjacency matrix also reveals some digit pairs that commonly confuse the model. Let‚Äôs find some mislabeled examples to see how fair they are.\n\n\nShow test samples with mistakes\nimport torchvision\nimport numpy as np\n\n# Get samples where mistakes were made\ndef wrong(dataloader):\n    for i, (images, labels) in enumerate(testloader):\n        for j, (image, label) in enumerate(zip(images, labels)):\n            label, pred = label.item(), y_pred[batch_size*i+j].item()\n            if pred != label:\n                yield (image, label, pred)\n\n# Plot several examples\nloader = wrong(testloader)\nfor _ in range(4):\n    image, label, pred = next(loader)\n    img = np.transpose(image, (1, 2, 0))\n    plt.imshow(img, cmap='Greys')\n    plt.title(f\"Actual: {label}\\nPredicted: {pred}\", size=42)\n    plt.xticks([],[])\n    plt.yticks([],[])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersonally, I think these are still very comprehensible, maybe just a bit unusual. Even though the model does not get these, it apparently does many more - which I think is fair to call a success."
  },
  {
    "objectID": "posts/4-classification.html#footnotes",
    "href": "posts/4-classification.html#footnotes",
    "title": "Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I‚Äôm running this code via GitHub actions, it runs on cpu. I don‚Äôt want to spend too long deploying, so I‚Äôve simplified the model accordingly.‚Ü©Ô∏é"
  }
]